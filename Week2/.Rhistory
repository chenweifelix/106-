1+1 ->
1+1
1+1
9%%5
render
knit()
```1+1
```9%%5
knitr::opts_chunk$set(echo = TRUE)
```{r}
9%%5
render
{r} [Click Me](https://chenweifelix.github.io/106-/Week1/Week1_hw.html)
insatall.ggplot2
install.packages(ggplot2)
install.packages(ggplot2)
iris
ggplot(data=iris, x= iris$Species, y= iris$Sepal.Length)
install.packages(ggplot2)
install.packages(ggplot2)
install.packages(ggplot2)
install.packages(ggplot2)
install.packages(ggplot2)
install.package(ggplot2)
install.packages(ggplot2)
library(ggplot)
library(ggplot2)
install.packages(ggplot2)
install.packages(ggplot2)
"ggplot2"
install.packages("ggplot2")
library(ggplot2)
library(ggplot2)
ggplot(data=iris, x= iris$Species, y= iris$Sepal.Length)
iris
ggplot(data=iris, x= iris$Species, y= iris$Sepal.Length)
iris
ggplot(data=iris, x= iris$Species, y= iris$Sepal.Length)
ggplot(data=iris, x= iris$Species, y= iris$Sepal.Length)
ggplot(data = iris)
ggplot(data = iris)
ggplot(data = iris, x = iris$Species, y=iris$Sepal.Length)
library(ggplot2)
library(ggplot2)
ggplot(data=iris, x= iris$Species, y= iris$Sepal.Length,geom_abline())
ggplot(data=iris, x= iris$Species, y= iris$Sepal.Length,geom_abline(aes()))
ggplot(data=iris, aes(x= iris$Species), aes(y= iris$Sepal.Length))
library(xml2)
library(tmcn)
source('pttTestFunction.R')
source('pttTestFunction.R')
id = c(1110:1118)
URL = paste0("https://www.ptt.cc/bbs/TOEIC/index", id, ".html")
filename = paste0(id, ".txt")
id = c(1110:1118)
URL = paste0("https://www.ptt.cc/bbs/TOEIC/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
library(xml2)
install.packages("xml2")
install.packages("tmcn")
install.packages("rvest")
setwd("C:/Users/user/Desktop/Academy/Programming/Github/106----_--------_---/Week2")
library(xml2)
library(tmcn)
library(rvest)
pttTestFunction <- function(URL, filename)
{
#URL   = "https://www.ptt.cc/bbs/NTUcourse/index.html"
html  = read_html(URL)
title = html_nodes(html, "a")
href  = html_attr(title, "href")
data = data.frame(title = toUTF8(html_text(title)),
href = href)
data = data[-c(1:10),]
getContent <- function(x) {
url  = paste0("https://www.ptt.cc", x)
tag  = html_node(read_html(url), 'div#main-content.bbs-screen.bbs-content')
text = toUTF8(html_text(tag))
}
#getContent(data$href[1])
allText = sapply(data$href, getContent)
allText
#out <- file(filename, "w", encoding="BIG-5")
write.table(allText, filename)
#close(out)
}
source('pttTestFunction.R')
source('pttTestFunction.R')
library(rvest)
pttTestFunction <- function(URL, filename)
{
#URL   = "https://www.ptt.cc/bbs/NTUcourse/index.html"
html  = read_html(URL)
title = html_nodes(html, "a")
href  = html_attr(title, "href")
data = data.frame(title = toUTF8(html_text(title)),
href = href)
data = data[-c(1:10),]
getContent <- function(x) {
url  = paste0("https://www.ptt.cc", x)
tag  = html_node(read_html(url), 'div#main-content.bbs-screen.bbs-content')
text = toUTF8(html_text(tag))
}
#getContent(data$href[1])
allText = sapply(data$href, getContent)
allText
#out <- file(filename, "w", encoding="BIG-5")
write.table(allText, filename)
#close(out)
}
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
readLines("1110.txt")
source('pttTestFunction.R')
id = c(1110:1118)
source('pttTestFunction.R')
pttTestFunction <- function(URL, filename)
{
#URL   = "https://www.ptt.cc/bbs/NTUcourse/index.html"
html  = read_html(URL)
title = html_nodes(html, "a")
href  = html_attr(title, "href")
data = data.frame(title = toUTF8(html_text(title)),
href = href)
data = data[-c(1:10),]
getContent <- function(x) {
url  = paste0("https://www.ptt.cc", x)
tag  = html_node(read_html(url), 'div#main-content.bbs-screen.bbs-content')
text = toUTF8(html_text(tag))
}
#getContent(data$href[1])
allText = sapply(data$href, getContent)
allText
#out <- file(filename, "w", encoding="BIG-5")
write.table(allText, filename)
#close(out)
}
id = c(1110:1118)
URL = paste0("https://www.ptt.cc/bbs/TOEIC/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
pttTestFunction(URL[1], filename[1])
source('C:/Users/user/Desktop/Academy/Programming/Github/106----_--------_---/Week2/pttTestFunction.R')
source('pttTestFunction.R')
source('pttTestFunction.R')
id = c(1110:1118)
URL = paste0("https://www.ptt.cc/bbs/TOEIC/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
source('pttTestFunction.R')
id = c(1110:1118)
URL = paste0("https://www.ptt.cc/bbs/TOEIC/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
version
debugSource('C:/Users/user/Desktop/Academy/Programming/Github/106----_--------_---/Week2/wordcloud.R')
f
debugSource('C:/Users/user/Desktop/Academy/Programming/Github/106----_--------_---/Week2/wordcloud.R')
debugSource('C:/Users/user/Desktop/Academy/Programming/Github/106----_--------_---/Week2/wordcloud.R')
read_html(URL[1])
html = read_html(URL[1])
html$doc
html$doc[1]
html = read_html("https://www.ptt.cc/bbs/TOEIC/M.1531032112.A.46B.html")
html
html_text(html)
a = html_text(html)
write.table(a, "test.txt")
write.table(a, "test.txt", encoding="UTF-8")
write.table(a, "test.txt", encoding="UTF8")
write.table(a, "test.txt", encoding="utf8")
write.table(a, "test.txt", fileEncoding="UTF-8")
write.table(a, "test.txt", fileEncoding="UTF-8")
write.table(a, "test.txt", fileEncoding="BIG-5")
a
write.table(a, "test.txt")
write.csv(a, "test.txt")
write.csv(a, "test.txt", fileEncoding = "UTF-8")
write.csv(a, "test.txt", fileEncoding = "BIG-5")
write.csv(a, "test.txt", encoding = "UTF-8")
write.csv(a, "test.txt", encoding = "UTF8")
help Encoding()
write.csv(a, "test.txt", encoding = "UTF-8-ROM")
out = file("test.txt". encoding="UTF-8")
out = file("test.txt". encoding="UTF-8")
out = file("test.txt", encoding="UTF-8")
write.table(a, file=out)
out = file("test.txt", encoding="BIG-5")
write.table(a, file=out)
out = file("test.txt", encoding="BIG-5")
write.table(a, file=out)
a = toUTF8(a)
a
a
html = read_html("https://www.ptt.cc/bbs/TOEIC/M.1531032112.A.46B.html")
a = html_text(html)
a
a = toUTF8(html_text(html))
a
a[1]
write.table(a, "text.txt")
library("jiebaR")
install.packages("jiebaR")
library("jiebaR")
Sys.setlocale(category = "LC_ALL", locale = "cht")
cc = worker()
cc
cc("Voices_in_My_Head.txt")
cc = worker()
cc[Voices]
cc["Voices_in_My_Head.txt"]
mlb <-table(cc["Voices_in_My_Head.txt"])
mlb
mlb <-table(cc["Voices_in_My_Head.segment.2018-07-11_15_33_55.txt"])
mlb
words = cc["Voices_in_My_Head.txt"]
mlb <-table(words)
mlb
cc = worker()
words = cc["Voices_in_My_Head.txt"]
words
table(words)
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
seg = lapply("Voices_in_My_Head.txt", jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[-c(1:34),]
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
debugSource('C:/Users/user/Desktop/Academy/Programming/Github/106----_--------_---/Week2/wordcloud.R', echo=TRUE)
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
freqFrame
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame
words = cc["Voices_in_My_Head.txt"]
words
table(words)
words
table(words)
words
class(words)
readLines("Voices_in_My_Head.segment.2018-07-11_15_41_58.txt")
readLines("Voices_in_My_Head.segment.2018-07-11_15_41_58.txt", encoding = "UTF-8")
words <- readLines("Voices_in_My_Head.segment.2018-07-11_15_41_58.txt", encoding = "UTF-8")
table(words)
mlb <-table(words)
mlb
words
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame
freqFrame = as.data.frame(table(unlist(words)))
table(words)
class(table(words))
freqFrame = as.data.frame(table(words))
freqFrame
words
text = readLines("Voices_in_My_Head.txt")
text
words = cc["天氣真好"]
words
words = cc[text]
words
table(words)
freqFrame = as.data.frame(table(words))
freqFrame = freqFrame[-c(1:34),]
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
View(freqFrame)
words
table(words)
"Voices_in_My_Head.txt" <- tm_map(docs, toSpace, "的")
library(tm)
install.packages("tm")
library(tm)
install.packages("tm")
library(tm)
"Voices_in_My_Head.txt" <- tm_map(docs, toSpace, "的")
"Voices_in_My_Head.txt" <- tm_map("Voices_in_My_Head.txt", toSpace, "的")
"Voices_in_My_Head.txt" <- tm_map("Voices_in_My_Head.txt", toSpace, "的")
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
"Voices_in_My_Head.txt" <- tm_map("Voices_in_My_Head.txt", toSpace, "的")
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
"Voices_in_My_Head.txt" <- tm_map("Voices_in_My_Head.txt", toSpace, "的")
table(words)
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply("Voices_in_My_Head.txt", jieba_tokenizer)
freqFrame = as.data.frame(table(words))
freqFrame = freqFrame[-c(1:34),]
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
freqFrame = freqFrame[-c(1:34),]
View(freqFrame)
freqFrame = freqFrame[-c(1:33),]
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
wordcloud(freqFrame$words,freqFrame$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
wordcloud(freqFrame$words,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
"Voices_in_My_Head.txt" <- tm_map("Voices_in_My_Head.txt", toSpace, "的")
words <- tm_map(words, toSpace, "的")
words <- tm_map(words, toSpace, "的")
words <- tm_map(words, toSpace("的"))
wordcloud(freqFrame$words,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=100,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
docs <- Corpus(VectorSource(words))
docs
docs <- tm_map(docs, toSpace, "的")
wordcloud(freqFrame$words,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=100,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
table(docs)
table(docs)
words
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(words))
freqFrame = freqFrame[-c(1:33),]
wordcloud(freqFrame$words,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=100,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
docs <- tm_map(docs, toSpace, "的")
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(words))
freqFrame = freqFrame[-c(1:33),]
wordcloud(freqFrame$words,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=100,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
docs <- tm_map(docs, toSpace, "我")
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(words))
freqFrame = freqFrame[-c(1:33),]
wordcloud(freqFrame$words,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=100,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
freqFrame = as.data.frame(table(docs))
freqFrame = as.data.frame(table(docs))
View(docs)
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[-c(1:33),]
wordcloud(freqFrame$words,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=100,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
wordcloud(freqFrame$words,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=100,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
View(freqFrame)
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=100,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
docs <- tm_map(docs, toSpace, "了")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "去")
docs <- tm_map(docs, toSpace, "而")
docs <- tm_map(docs, toSpace, "也")
docs <- tm_map(docs, toSpace, "們")
docs <- tm_map(docs, toSpace, "又")
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[-c(1:33),]
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=100,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
docs <- tm_map(docs, toSpace, "是")
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[-c(1:33),]
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=65,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
cc = worker()
text = readLines("Voices_in_My_Head.txt")
words = cc[text]
words
cc = worker()
text = readLines("Voices_in_My_Head.txt")
words = cc[text]
words
install.packages("wordcloud")
library(wordcloud)
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
install.packages("wordcloud")
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[-c(1:33),]
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=1,max.words=65,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
install.packages("forecast", repos = "http://cran.us.r-project.org")
install.packages("tm")
install.packages("tm")
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
docs <- Corpus(VectorSource(words))
docs
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "了")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "去")
docs <- tm_map(docs, toSpace, "而")
docs <- tm_map(docs, toSpace, "也")
docs <- tm_map(docs, toSpace, "們")
docs <- tm_map(docs, toSpace, "又")
docs <- tm_map(docs, toSpace, "是")
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
