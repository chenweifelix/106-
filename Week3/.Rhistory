data = read.csv("data.csv")
data
View(data)
View(data)
table = table(data)
View(table)
data = read.csv("data.csv")
View(data)
data$mean
data$mean = (data$score)/25
data$mean
View(data)
read.table("artiemocionEJPA-final1.sav")
View(data)
names(mean) <- "score_mean"
names(data$mean) <- "score_mean"
View(data)
`colnames<-`("mean"", "score_mean")
colnames<-("mean"", "score_mean")
colnames<-("mean", "score_mean")
data$mean <- names<-("score_mean")
View(data)
data$mean = (data$score)/25
data$mean
data$mean <- names("score_mean")
View(data)
data$mean = (data$score)/25
data$mean <- names("score_mean")
View(data)
data$mean = (data$score)/25
View(data)
data$mean <- names("score_mean")
data$score_mean = (data$score)/25
View(data)
data$age_mean <- (data$age)/400
View(data)
data$score_mean = sum(data$score)/25
View(data)
data$age_mean <- sum(data$age)/400
View(data)
data$age_mean <- sum(data$age)/400
data$sum_age_score <- (data$score) + (data$age)
View(data)
library(ggplot2)
library(readr)
install.packages(readr)
install.packages("readr"")
install.packages("readr)
install.packages("readr")
library(ggplot2)
library(readr)
library(scales)
library(plotly)
install.packages("plotly")
install.packages("plotly")
library(dplyr)
system("ls ../input")
lang<-read.csv("../input/data_languages.csv",stringsAsFactors = F)
lang<-read.csv("../input/data_languages.csv",stringsAsFactors = F)
setwd("C:/Users/user/Desktop/Academy/Programming/Github/106----_--------_---/Week3")
lang<-read.csv("../input/data_languages.csv",stringsAsFactors = F)
system("ls ../input")
lang<-read.csv("../input/data_languages.csv",stringsAsFactors = F)
world<-map_data("world")
world<-map_data("world")
install.packages("readr")
install.packages("plotly")
library(ggplot2)
library(readr)
library(scales)
library(plotly)
library(dplyr)
system("ls ../input")
lang<-read.csv("../input/data_languages.csv",stringsAsFactors = F)
world<-map_data("world")
install.packages("map")
install.packages("map")
world<-map_data("world")
install.packages("map")
("world")
world<-map_data("world")
install.packages("maps")
world<-map_data("world")
ggplot()+
geom_point(data=lang,aes(Longitude,Latitude,color=Degree.of.endangerment),alpha=0.5,size=1)+
geom_path(data=world,mapping = aes(long,lat,group=group),size=0.2)+
guides(color=guide_legend(override.aes = list(alpha=1,size=5)))+
scale_color_brewer(palette = "Set1")+ggtitle("Extinct Languages based on Degree of Endangerment")
lang<-read.csv("../input/data_languages.csv",stringsAsFactors = F)
d <- read.csv("data_languages.csv")
ggplot()+
geom_point(data=d,aes(Longitude,Latitude,color=Degree.of.endangerment),alpha=0.5,size=1)+
geom_path(data=world,mapping = aes(long,lat,group=group),size=0.2)+
guides(color=guide_legend(override.aes = list(alpha=1,size=5)))+
scale_color_brewer(palette = "Set1")+ggtitle("Extinct Languages based on Degree of Endangerment")
ggplotly()
ggplotly()
ggplot()+
geom_point(data=d,aes(Longitude,Latitude,color=Degree.of.endangerment),alpha=0.5,size=1)+
geom_path(data=world,mapping = aes(long,lat,group=group),size=0.2)+
guides(color=guide_legend(override.aes = list(alpha=1,size=5)))+
scale_color_brewer(palette = "Set1")+ggtitle("Extinct Languages based on Degree of Endangerment")
lang<-read.csv("../input/data.csv",stringsAsFactors = F)
country_langex<-d%>%
select(Countries)%>%
group_by(Countries)%>%
summarise(total=n())%>%
arrange(desc(total))
ggplot(data=country_langex[1:15,],aes(x=reorder(as.factor(Countries),total),y=total))+
geom_bar(stat="identity",fill="tomato")+scale_x_discrete("Countries")+
scale_y_continuous("No of languages")+
ggtitle("Top 15 Countries with highest number of vulnerable languages")+
geom_text(aes(label=total),vjust=0.8,color="darkgreen",size=3)+
theme(axis.text.x = element_text(angle=75,vjust=0.5))
zero_lang<-d%>%
select(Countries,Latitude,Longitude,Number.of.speakers,Name.in.English)%>%
filter(Number.of.speakers==0)
zero_lang$Name.in.English<-as.factor(zero_lang$Name.in.English)
world2<-map_data("world")
ggplot()+
geom_point(data=zero_lang,aes(Longitude,Latitude),color="red",alpha=0.9)+
geom_path(data=world2,mapping = aes(long,lat,group=group),size=0.2)+
ggtitle("Languages with zero speakers")
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(XML)
install.packages("XML")
library(XML)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
data <- list()
from <- 1
to <- 10
prefix = "https://www.psychologytoday.com/intl/topics/self-help?"
data <- list()
data <- rbind( data, as.matrix(paste('https://www.psychologytoday.com/', url.list, sep='')) )
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.psychologytoday.com/', url.list, sep='')) )
{
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.psychologytoday.com/', url.list, sep='')) )
}
data <- unlist(data)
head(data)
url  <- paste0( prefix, as.character(id), "intl/topics/self-help?page=" )
from <- 1
to <- 10
prefix = "https://www.psychologytoday.com/"
data <- list()
for( id in c(from:to) )
url  <- paste0( prefix, as.character(id), "intl/topics/self-help?page=" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.psychologytoday.com/', url.list, sep='')) )
{
url  <- paste0( prefix, as.character(id), "intl/topics/self-help?page=" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.psychologytoday.com/', url.list, sep='')) )
}
data <- unlist(data)
head(data)
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
#print(hour)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
part <- strsplit( temp, split=" ", fixed=T )
source('~/.active-rstudio-document', echo=TRUE)
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
#print(hour)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
#print(hour)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
#print(hour)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
sapply(data, getdoc)
from <- 1
to <- 10
prefix = "https://www.psychologytoday.com/"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), "intl/topics/self-help?page=" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.psychologytoday.com/', url.list, sep='')) )
}
data <- unlist(data)
library(bitops)
library(httr)
library(RCurl)
install.packages("XML")
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
from <- 1
to <- 10
prefix = "https://www.psychologytoday.com/"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), "intl/topics/self-help?page=" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.psychologytoday.com/', url.list, sep='')) )
}
data <- unlist(data)
head(data)
url  <- ("https://www.reddit.com/r/psychologystudents/")
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.reddit.com/r/psychologystudents/', url.list, sep='')) )
data <- rbind( data, as.matrix('https://www.reddit.com/r/psychologystudents/', url.list, sep='') )
data <- unlist(data)
head(data)
data <- list()
url  <- ("https://www.reddit.com/r/psychologystudents/")
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix('https://www.reddit.com/r/psychologystudents/', url.list, sep='') )
data <- unlist(data)
head(data)
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
#print(hour)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
###
d.corpus <- Corpus( DirSource("./DATA") )
###
d.corpus <- Corpus( DirSource("./DATA") )
d.corpus <- tm_map(d.corpus, removePunctuation)
#print(hour)
name <- paste0('./DATA/', hour, ".txt")
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
#print(hour)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
write(doc, name, append = TRUE)
#print(hour)
name <- paste0('./DATA/', hour, ".txt")
sapply(data, getdoc)
prefix = "https://www.psychologytoday.com/"
data <- list()
url  <- ("https://www.reddit.com/r/psychologystudents/")
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix('https://www.reddit.com/r/psychologystudents/', url.list, sep='') )
data <- unlist(data)
head(data)
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
#print(hour)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
###
d.corpus <- Corpus( DirSource("./DATA") )
sapply(data, getdoc)
part
temp
time
html
#print(hour)
name <- "psy.txt"
write(doc, name, append = TRUE)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
#print(hour)
name <- "psy.txt"
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
doc
html
url  <- ("https://www.reddit.com/r/psychologystudents/")
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix('https://www.reddit.com/r/psychologystudents/', url.list, sep='') )
data <- unlist(data)
head(data)
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
html
html <- htmlParse( getURL(url) )
html
doc  <- xpathSApply( html, "//p", xmlValue )
doc
sapply(data, getdoc)
###
d.corpus <- Corpus( DirSource("./DATA") )
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//p", xmlValue )
#print(hour)
name <- "psy.txt"
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
head(data)
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//p", xmlValue )
#print(hour)
name <- "psy.txt"
write(doc, name, append = TRUE)
}
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//p", xmlValue )
#print(hour)
name <- "psy.txt"
write(doc, name, append = TRUE)
sapply(data, getdoc)
###
d.corpus <- Corpus( DirSource("./DATA") )
name
sapply(data, getdoc)
###
d.corpus <- Corpus( DirSource("./DATA") )
###
d.corpus <- Corpus( DirSource("./Data") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
as.data.frame(table(d))
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
n = length(seg)
TDM = tokens[[1]]
colNames <- names(seg)
colNames <- gsub(".txt", "", colNames)
for( id in c(2:n) )
{
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
TDM[is.na(TDM)] <- 0
library(knitr)
kable(head(TDM))
mixseg = worker()
unlist( segment(d[[1]], mixseg) )
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
n = length(seg)
TDM = tokens[[1]]
colNames <- names(seg)
colNames <- gsub(".txt", "", colNames)
for( id in c(2:n) )
{
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
library(knitr)
id
